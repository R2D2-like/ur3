ur3e_gym: #namespace
    # General Agent parameters
    env_id: "UR3ePegInHoleEnv-v0"
    controller_type: "parallel_position_force" # or "admittance"
    driver: "gazebo" # or "robot" or "old_driver"
    ft_sensor: True # use force sensor
    relative_to_ee: False # consider end-effector translation with respect to the end-effector frame and not the world frame
    robot_control_dt: 0.002 # control frequency for the low-level controller 500hz
    agent_control_dt: 0.05 # control frequency for the policy actions 20hz
    reset_time: 1.0 # desired time to performe reset to initial position

    steps_per_episode: 300 # max number of steps per episode
    
    ft_hist: True # retrieve history of ft instead of just the latest reading
    actor_class: "wave" # network architecture "wave" or "default"

    rand_seed: 10
    
    # Simulation Parameters
    gazebo_models: True
    gazebo_model_stiffness: "random"
    gazebo_stiff_upper_limit: 1.0e+5 
    gazebo_stiff_lower_limit: 7.0e+4
    gazebo_models_episode_num: 5


    # initial conditions
    random_initial_pose: True
    rand_init_interval: 1
    # for each dimension define upper and lower bound from initial pose
    workspace: [[0.04,-0.04], [0.04, -0.04], [0.005, 0.02], [10, -10], [10, -10], [10, -10]]
    
    # Target pose
    target_pos: [-0.12986, -0.23504, 0.50221, -0.00807, 0.74175, -0.66452, 0.09028]
    extra_ee: [0, 0, 0.05, 0, 0, 0, 1]
    end_effector_points: [[0.,0.,0.]]
    target_pose_uncertain: False
    uncertainty_std: [0.001, 10.0] # +- position error, +- rotation angle error
    fixed_uncertainty_error: False # use random or fixed uncertainty error
    # uncertainty_std: [0.0, 0.0, 0.0, 0.0, 0.0, 10.0] # individual directions

    changable_goal: True
    goal_changer_interval: 15
    goals: [
        [3.2317, -1.979, 1.3969, -0.4844, -0.1151, -1.7565],
        # [-2.24462, -1.67999,  1.891  , -3.48903,  5.38531,  0.09558],
        # [-3.05932, -1.85342,  1.42101,  0.40562,  0.46215,  0.19672],
    ]
    
    # actions parameters 
    n_actions: 24 

    # Reward parameters
    target_dims: [0,1,2,3,4,5]
    distance_threshold: 2.0
    reward_type: 'force'
    cost_positive: True
    cost:
        l1: 1.0
        l2: 10.0
        alpha: 0.00001
        ws: [1.0,1.0,0.0] # weights of distance, force, action components
        goal: 100 # reward on success
        speed_violation: -1.0
        ik_violation: -1.0
        collision: -50.0

# Controller parameters
ur3e_force_control:
    n_actions: 24
    robot_control_dt: 0.002
    
    max_speed: [0.02,0.02,0.02,0.1,0.1,0.1] # per agent action
    action_scale: [1., 1., 1., 1., 1., 1.]

    max_force_torque: [40.0, 40.0, 40.0, 2, 2, 2]
    desired_force_torque: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    
    # parallel control parameters
    pd_range_type: 'mult' # or 'sum'
    base_position_kp: [3.0, 3.0, 3.0, 5.0, 5.0, 5.0]
    kpd_range: 2
    base_force_kp: [5.0e-3, 5.0e-3, 5.0e-3, 2.5e-1, 2.5e-1, 2.5e-1]
    kpi_range: 2
    
    alpha: [0.5,0.5,0.5,0.5,0.5,0.5] # keep a degree of compliance
    alpha_base: 0.5
    alpha_range: 0.4
