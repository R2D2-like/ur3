ur3e_gym: #namespace
    # Agent parameters
    env_id: "UR3eParallelMidpointsEnv-v1"
    controller_type: "parallel_position_force"
    driver: "gazebo"
    ft_sensor: True
    relative_to_ee: False
    agent_control_dt: 0.05
    reset_time: 1.0

    steps_per_episode: 250
    
    gazebo_models: False
    gazebo_model_stiffness: random

    # initial conditions
    random_initial_pose: True
    target_pose_uncertain: True
    uncertainty_std: [0.001, 0.0]
    # for each dimension define upper and lower bound from initial pose
    workspace: [[0.03,-0.03], [0.005, 0.03], [0.03, -0.03], [5, -5], [45, -45], [5, -5]]
    init_q: [1.93048, -2.13818, -1.21322,  0.36888,  1.05412,  1.09136]
    reset_upper_sequence: [[1.66066, -1.96887, -0.75349, -1.11645,  1.4963 ,  0.0467],
    [1.68087, -1.76652, -1.47528, -0.14633,  1.45902,  0.0154]]
    reset_lower_sequence: [[1.65974, -2.05772, -1.02218, -0.28533,  1.47796,  0.00901], 
        [1.65447, -2.10598, -0.69243, -0.88211,  1.49305,  0.03213]]
    reset_inner_sequence: [[1.66401, -2.10419, -1.11669,  0.06624,  1.47132, -0.00942]]

    reset_upper_height: -0.56156633
    reset_lower_height: -0.57706141
    target_midpoints: [[-0.1002317 , -0.47915622,  0.39579533, -0.00382968,  0.88005645, -0.4748363 , -0.00405845],
    [-0.10017571, -0.4933103 ,  0.39571364, -0.00388745,  0.88011455, -0.47472849, -0.00401635],
    [-0.10009617, -0.51294806,  0.39560425, -0.00396703,  0.88019508, -0.47457899, -0.00395773],
    [-0.10000094, -0.52204305,  0.40693015, -0.00406577,  0.88029694, -0.47438979, -0.00388457],
    [-0.09993238, -0.52697531,  0.41249584, -0.00397033,  0.859416  , -0.51124589, -0.00400195],
    [-0.09984483, -0.53211655,  0.41582683, -0.00387182,  0.83432523, -0.55124346, -0.00413101],
    [-0.09975851, -0.5359664 ,  0.42095208, -0.00357787,  0.78086991, -0.62466789, -0.00440506],
    [-0.09967483, -0.53612947,  0.42593643,  0.00246403,  0.71597797, -0.69811581,  0.00194366],
    [-0.10025745, -0.53657242,  0.43617077, -0.00405397,  0.71369152, -0.70043642, -0.00409995]
    ]
    target_pos: [-0.10025745, -0.53657242,  0.43617077, -0.00405397,  0.71369152, -0.70043642, -0.00409995] 
    extra_ee: [0, 0, 0.1, 0, 0, 0, 1]
    end_effector_points: [[0.,0.,0.]]
    
    # actions parameters 
    n_actions: 14 # 6 positions force + 2 kp parameters + 2 alphas
    
    # Reward parameters
    target_dims: [0,1,2,3,4,5]
    midpoint_dist_threshold: 8.0
    distance_threshold: 1.0
    reward_type: 'force'
    cost_positive: True
    max_action: 14
    cost:
        l1: 1.0
        l2: 10.0
        ws: [1.0,0.3,0.5]
        alpha: 0.00001
        goal: 100
        step: 0.0
        speed_violation: -1.0
        ik_violation: -1.0
        collision: -50.0
        
# Controller parameters
ur3e_force_control:
    robot_control_dt: 0.002
    max_speed: [0.01,0.01,0.01,0.01,0.01,0.01]
    max_force_torque: [40.0, 40.0, 40.0, 2, 2, 2]
    desired_force_torque: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    action_scale: [1., 1., 1., 1., 1., 1.]
        
    # Hybrid control parameters
    base_position_kp: 800
    position_pd_exp_base: [8.0e-3, 8.0e-3, 8.0e-3, 5.0e-2, 5.0e-2, 5.0e-2]
    kpd_range: 700
    base_force_kp: 700
    force_pd_exp_base: [1.0e-5, 1.0e-5, 1.0e-5, 1.0e-5, 1.0e-5, 1.0e-5]
    kpi_range: 600
    
    alpha: [0.7,0.7,0.7,0.7,0.7,0.7] # keep a degree of compliance
    alpha_base: 0.5
    alpha_range: 0.4
